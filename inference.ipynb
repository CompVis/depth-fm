{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <h2 align=\"center\">📻 DepthFM: Fast Monocular Depth Estimation with Flow Matching</h2>\n",
    " <p align=\"center\"> \n",
    "    Ming Gui<sup>*</sup> · Johannes S. Fischer<sup>*</sup> · Ulrich Prestel · Pingchuan Ma\n",
    " </p><p align=\"center\"> \n",
    "    Dmytro Kotovenko · Olga Grebenkova · Stefan A. Baumann · Vincent Tao Hu · Björn Ommer\n",
    " </p>\n",
    " <p align=\"center\"> \n",
    "    <b>CompVis Group, LMU Munich</b>\n",
    " </p>\n",
    "  <p align=\"center\"> <sup>*</sup> <i>equal contribution</i> </p>\n",
    "</p>\n",
    "\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "from depthfm import DepthFM\n",
    "from pathlib import Path\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(img_abs_path):\n",
    "    for entry in img_abs_path.iterdir():\n",
    "        if entry.suffix == \".jpeg\":\n",
    "            yield entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_max_res(\n",
    "    img: Image.Image, max_edge_resolution: int, resample_method=Resampling.BILINEAR\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize image to limit maximum edge length while keeping aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        img (`Image.Image`):\n",
    "            Image to be resized.\n",
    "        max_edge_resolution (`int`):\n",
    "            Maximum edge length (pixel).\n",
    "        resample_method (`PIL.Image.Resampling`):\n",
    "            Resampling method used to resize images.\n",
    "\n",
    "    Returns:\n",
    "        `Image.Image`: Resized image.\n",
    "    \"\"\"\n",
    "    original_width, original_height = img.size\n",
    "    downscale_factor = min( max_edge_resolution / original_width, max_edge_resolution / original_height)\n",
    "\n",
    "    new_width  = int(original_width * downscale_factor)\n",
    "    new_height = int(original_height * downscale_factor)\n",
    "\n",
    "    new_width  = round(new_width / 64) * 64\n",
    "    new_height = round(new_height / 64) * 64\n",
    "\n",
    "    # print(f\"Resizing image from {original_width}x{original_height} to {new_width}x{new_height}\")\n",
    "\n",
    "    resized_img = img.resize((new_width, new_height), resample=resample_method)\n",
    "    return resized_img, (original_width, original_height), (new_width, new_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(im_fp: str) -> torch.Tensor:\n",
    "    # set image filepath\n",
    "    # im_fp = 'assets/towers/5H7UICByzy_sml.jpeg'\n",
    "    \n",
    "    # open the image\n",
    "    im = Image.open(im_fp).convert('RGB')\n",
    "    \n",
    "    processing_res = 640\n",
    "    im, orig_res, new_res = resize_max_res(im, processing_res)\n",
    "    \n",
    "    # convert to tensor and normalize to [-1, 1] range\n",
    "    x = np.array(im)\n",
    "    x = einops.rearrange(x, 'h w c -> c h w')\n",
    "    x = x / 127.5 - 1\n",
    "    x = torch.tensor(x, dtype=torch.float32)[None]\n",
    "    \n",
    "    # print(f\"{'Shape':<10}: {x.shape}\")\n",
    "    # print(f\"{'dtype':<10}: {x.dtype}\")\n",
    "    \n",
    "    # display(im.resize(new_res))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_depth(model: DepthFM, x: torch.Tensor) -> torch.Tensor:\n",
    "    dev = 'cuda:0'\n",
    "    model = model.to(dev)\n",
    "    depth = model.predict_depth(x.to(dev), num_steps=4, ensemble_size=12)\n",
    "    \n",
    "    # print(f\"{'Depth':<10}: {depth.shape}\")\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    min_val, max_val = np.min(arr), np.max(arr)\n",
    "    arr_normalized = (arr - min_val) / (max_val - min_val)  # Normalize to [0, 1]\n",
    "    arr_normalized = (arr_normalized * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    return arr_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(depth: torch.Tensor):\n",
    "    plt.imshow(depth.squeeze().cpu().numpy(), cmap='turbo')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply color map & convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_save(arrd: np.array) -> Image:\n",
    "    arr = depth.squeeze().numpy(force=True)\n",
    "    arr_normalized = normalize(arr)\n",
    "    colored = cv2.applyColorMap(arr_normalized, cv2.COLORMAP_JET)\n",
    "    im = Image.fromarray(cv2.cvtColor(colored, cv2.COLOR_BGR2RGB))\n",
    "    im = im.convert('RGB')\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DepthFM('checkpoints/depthfm-v1.ckpt')  # load model\n",
    "# im_fp = 'assets/towers/5H7UICByzy_sml.jpeg' # set image filepath\n",
    "img_rel_path = 'assets/towers'\n",
    "img_abs_path = Path.cwd() / img_rel_path\n",
    "new_save_folder = img_abs_path.parent / Path(str(img_abs_path.name) + '_depth')\n",
    "Path(new_save_folder).mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "for im_fp in generator(img_abs_path):\n",
    "    x = convert_to_tensor(im_fp)  # convert image to tensor\n",
    "    depth = infer_depth(model, x)  # infer image depths\n",
    "    # visualize(depth)  # show visualization\n",
    "    im = prep_for_save(arr)\n",
    "\n",
    "    # save\n",
    "    new_save_name = Path(str(im_fp.stem) + '_depth')\n",
    "    save_path = new_save_folder / new_save_name.with_suffix('.jpeg')\n",
    "    im.save(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
